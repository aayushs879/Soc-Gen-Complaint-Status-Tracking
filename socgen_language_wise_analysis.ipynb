{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "socgen2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aayushs879/Soc-Gen-Complaint-Status-Tracking/blob/master/socgen_language_wise_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Pa7g0BMsDdgQ",
        "colab_type": "code",
        "outputId": "3c07d621-9436-4481-e2d3-8bde0b76b869",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "#mounting colab book at google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "import os #better data flow\n",
        "path = os.path.abspath('gdrive/My Drive/HackerEarth/Soc-Gen')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "x5sS2ikbDzjV",
        "colab_type": "code",
        "outputId": "e37a9254-d133-4a23-8db5-b4509c7940ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "#importing the libraries and downloading stopwords\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split as tts\n",
        "from sklearn.metrics import f1_score as f1s"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DSIdfAXQFNNa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#importing the dataset and test set which is to be submitted\n",
        "train = pd.read_csv(os.path.join(path, 'train.csv'))\n",
        "test = pd.read_csv(os.path.join(path, 'test.csv'))\n",
        "\n",
        "#extracting and eliminating the target variable\n",
        "y = train['Complaint-Status'].iloc[:].values\n",
        "train = train.drop(['Complaint-Status'], axis = 1)\n",
        "\n",
        "#merging the training set with the test set for preprocesing\n",
        "data = train.append(test)\n",
        "train_length = len(train)\n",
        "\n",
        "#we will eb deleting variables which are no longer required to not eat up so much memory\n",
        "del train\n",
        "del test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UDaPOTW3JmQd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#print('head', data.head())\n",
        "#print('unique values', data.nunique())\n",
        "#print('null values', data.isnull().sum())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DuNjro1rKApR",
        "colab_type": "code",
        "outputId": "d4fe318e-9a19-44e6-9a07-7113accb0a2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        }
      },
      "cell_type": "code",
      "source": [
        "#analyzing the column with too much of null values\n",
        "z = pd.value_counts(data['Company-response'].iloc[:].values)\n",
        "print(z.keys())"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['Company has responded to the consumer and the CFPB and chooses not to provide a public response',\n",
            "       'Company chooses not to provide a public response',\n",
            "       'Company believes it acted appropriately as authorized by contract or law',\n",
            "       'Company believes the complaint is the result of a misunderstanding',\n",
            "       'Company disputes the facts presented in the complaint',\n",
            "       'Company believes complaint caused principally by actions of third party outside the control or direction of the company',\n",
            "       'Company believes complaint is the result of an isolated error',\n",
            "       'Company can't verify or dispute the facts in the complaint',\n",
            "       'Company believes complaint represents an opportunity for improvement to better serve consumers',\n",
            "       'Company believes complaint relates to a discontinued policy or procedure'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bJIzBFtOLV-M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#it seems that only top three have significant numbers so reducing the possible values of this column to 5 values i.e\n",
        "# 0 if it has none of the top 3\n",
        "# 1 if it has the first statement\n",
        "# 2 if second\n",
        "# 3 if third\n",
        "# 4 if value was not known preiously\n",
        "\n",
        "d1 = np.array((data['Company-response'] == z.keys()[0]).astype(np.int))\n",
        "d2 = 2*np.array((data['Company-response'] == z.keys()[1]).astype(np.int))\n",
        "d3 = 3*np.array((data['Company-response'] == z.keys()[2]).astype(np.int))\n",
        "data['Company-response'] = d1 + d2 + d3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "941vEq15OcTA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data['Company-response'] = data['Company-response'].fillna(4)\n",
        "data['Consumer-disputes'] = data['Consumer-disputes'].fillna('unknown')# filling missing values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZU11p0ooP1Uq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#the complainnt status certainly is not influenced by the dates they were sent but it may be influenced by the time it was delayed while being sent to the company\n",
        "\n",
        "c = []\n",
        "a1 = data['Date-sent-to-company'].iloc[:].values\n",
        "b1 = data['Date-received'].iloc[:].values\n",
        "for i in range(len(data)):\n",
        "  a = str(a1[i]).split('/')\n",
        "  b = str(b1[i]).split('/')\n",
        "  c.append(int(a[2])*365 + int(a[1])*30 + int(a[0]) - int(b[2])*365 -int(b[1])*30 -int(b[0]))\n",
        "    \n",
        "    \n",
        "data['delayed'] = c\n",
        "data = data.drop(['Date-received', 'Date-sent-to-company'], axis = 1)\n",
        "del a, a1\n",
        "del b, b1\n",
        "del c"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cElGrfFPRP_I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# detecting the corresponding languages of summary\n",
        "\n",
        "text = data['Consumer-complaint-summary'].iloc[:].values\n",
        "\n",
        "\"\"\"!pip install langdetect\n",
        "from langdetect import detect\n",
        "languages = []\n",
        "for i in range(len(text)):\n",
        "  languages.append(detect(text[i]))\n",
        "  \n",
        "  \n",
        "pd.DataFrame(languages, index = None).to_csv(os.path.join(path, 'languages.csv'), header = None, index = None)\n",
        "\"\"\"\n",
        "languages = pd.read_csv(os.path.join(path, 'languages.csv'), header = None)\n",
        "languages.columns = ['one']\n",
        "fr = {}\n",
        "en = {}\n",
        "es = {}\n",
        "for i in range(len(languages.one)):\n",
        "  if languages.one[i] == 'fr':\n",
        "    fr.update({i:text[i]})\n",
        "  elif languages.one[i] == 'es':\n",
        "    es.update({i:text[i]})\n",
        "  else:\n",
        "    en.update({i:text[i]})\n",
        "    \n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3tvdR9HlRWcH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "del d1\n",
        "del d2 \n",
        "del d3\n",
        "del z"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "URFcqAbfVYlr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# langdetect detected more than three languages but we will consider these as main as their counts were substantially large, rest we shall append to english language\n",
        "# now we make a dictionary of the statements corresponding to a particular language whilst preserving their indices\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "encor = {}\n",
        "frcor = {}\n",
        "escor = {}\n",
        "\n",
        "swfr = set(stopwords.words('french'))\n",
        "swen = set(stopwords.words('english'))\n",
        "swes = set(stopwords.words('spanish'))\n",
        "\n",
        "\n",
        "#making three different corpora for languages\n",
        "for key in fr:\n",
        "  words = fr[key].lower()\n",
        "  words = words.split()\n",
        "  ps = PorterStemmer()\n",
        "  words = [ps.stem(word) for word in words if not word in swfr]\n",
        "  sentence = ' '.join(words)\n",
        "  frcor.update({key:sentence})\n",
        "\n",
        "  \n",
        "for key in en:\n",
        "  words = en[key].lower()\n",
        "  words = words.split()\n",
        "  ps = PorterStemmer()\n",
        "  words = [ps.stem(word) for word in words if not word in swen]\n",
        "  sentence = ' '.join(words)\n",
        "  encor.update({key:sentence})\n",
        "\n",
        "  \n",
        "for key in es:\n",
        "  words = es[key].lower()\n",
        "  words = words.split()\n",
        "  ps = PorterStemmer()\n",
        "  words = [ps.stem(word) for word in words if not word in swes]\n",
        "  sentence = ' '.join(words)\n",
        "  escor.update({key:sentence})\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KlQOTOOaNWQs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "del en\n",
        "del fr\n",
        "del es"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bXxnCNpUDc73",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "285c4f40-cc40-44d2-82f4-6508b40216ed"
      },
      "cell_type": "code",
      "source": [
        "'''for key in encor:\n",
        "  text[int(key)] = encor[key]\n",
        "  \n",
        "for key in frcor:\n",
        "  text[int(key)] = frcor[key]\n",
        "  \n",
        "for key in escor:\n",
        "  text[int(key)] = escor[key]'''"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'for key in encor:\\n  text[int(key)] = encor[key]\\n  \\nfor key in frcor:\\n  text[int(key)] = frcor[key]\\n  \\nfor key in escor:\\n  text[int(key)] = escor[key]'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "metadata": {
        "id": "UQaB-dJ4Mwop",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# since counts of english are larger hence they get more words in their bag of words model\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "encv = CountVectorizer(max_features = 4000)# english\n",
        "frcv = CountVectorizer(max_features = 3000)#french\n",
        "escv = CountVectorizer(max_features = 3000)#spanish"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NCqd9orhNtLF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# we also append the language column to the main dataset so our model will know which language its currently dealing with\n",
        "\n",
        "data['languages'] = languages \n",
        "data = data.drop(['Consumer-complaint-summary'], axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RabJXZ1IWIZh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# encoding the categorical features into numerical values\n",
        "\n",
        "le = LabelEncoder()\n",
        "for column in data.columns:\n",
        "  data[column] = data[column].astype(str)\n",
        "data = data.apply(le.fit_transform)\n",
        "del languages"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3Nin1p-rVw2P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#creating our bow model\n",
        "x1 = encv.fit_transform(encor.values()).toarray()\n",
        "x2 = frcv.fit_transform(frcor.values()).toarray()\n",
        "x3 = escv.fit_transform(escor.values()).toarray()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CHxVvM9nz6Hn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#this is done to maintain homogenity across our matrix and separate out these models as three different ones so they dont overlap\n",
        "x1 = np.concatenate((x1, np.zeros((x1.shape[0], 6000))), axis = 1)\n",
        "x2 = np.concatenate((np.zeros((x2.shape[0], 4000)), x2, np.zeros((x2.shape[0], 3000))), axis=1)\n",
        "x3 = np.concatenate((np.zeros((x3.shape[0], 7000)), x3), axis = 1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nenjspitXBSR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# while fitting count vectorizer they were converted to numpy arrays, now we have to return those to their original indices\n",
        "X = np.empty((len(x1)+ len(x2) + len(x3), 10000))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CGfaVcAgVqqE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#extracting keys of our corpus\n",
        "\n",
        "encor = encor.keys()\n",
        "frcor = frcor.keys()\n",
        "escor = escor.keys()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jZJuwSbBWxGf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# the following block of code returns the statements to their oringinal indices\n",
        "i = 0\n",
        "for key in encor:\n",
        "  X[int(key)] = x1[i]\n",
        "  i+=1\n",
        "\n",
        "i = 0\n",
        "for key in frcor:\n",
        "  X[int(key)] = x2[i]\n",
        "  i+=1\n",
        "  \n",
        "i = 0\n",
        "for key in escor:\n",
        "  X[int(key)] = x3[i]\n",
        "  i+=1\n",
        "  \n",
        "np.savetxt(os.path.join(path, 'bow.txt'), X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_I5c1YFKXEe2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#concatenating the bag of words model and normal features\n",
        "del x1, x2, x3\n",
        "data = np.concatenate((data, X), axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WrsXbR0PXmIJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "del X\n",
        "np.savetxt(os.path.join(path, 'features.txt'), data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qZV_OG4oXWBk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "outputId": "16d259ad-6804-4b85-8620-38329f9657e9"
      },
      "cell_type": "code",
      "source": [
        "#no matter how many variables i delete colab runs out of memory while training so we download features and will train them in a different session\n",
        "from google.colab import files\n",
        "files.download(os.path.join(path, 'features.txt'))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------\n",
            "Exception happened during processing of request from ('::ffff:127.0.0.1', 48582, 0, 0)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 317, in _handle_request_noblock\n",
            "    self.process_request(request, client_address)\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 348, in process_request\n",
            "    self.finish_request(request, client_address)\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 361, in finish_request\n",
            "    self.RequestHandlerClass(request, client_address, self)\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 721, in __init__\n",
            "    self.handle()\n",
            "  File \"/usr/lib/python3.6/http/server.py\", line 418, in handle\n",
            "    self.handle_one_request()\n",
            "  File \"/usr/lib/python3.6/http/server.py\", line 406, in handle_one_request\n",
            "    method()\n",
            "  File \"/usr/lib/python3.6/http/server.py\", line 639, in do_GET\n",
            "    self.copyfile(f, self.wfile)\n",
            "  File \"/usr/lib/python3.6/http/server.py\", line 800, in copyfile\n",
            "    shutil.copyfileobj(source, outputfile)\n",
            "  File \"/usr/lib/python3.6/shutil.py\", line 82, in copyfileobj\n",
            "    fdst.write(buf)\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 800, in write\n",
            "    self._sock.sendall(b)\n",
            "ConnectionResetError: [Errno 104] Connection reset by peer\n",
            "----------------------------------------\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "2YudZ9wWX_Aa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "m"
      ]
    }
  ]
}